{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03a_ClassificaçãoTextos-BoW-TF-IDF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valmirf/mineracao_textual/blob/main/03a_Classifica%C3%A7%C3%A3oTextos_BoW_TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x9LVwe-LiMi"
      },
      "source": [
        "# Aprendizado Bayesiano: Naive Bayes\n",
        "\n",
        "O algoritmo Naive Bayes, assim como o KNN é um algoritmo de aprendizado com implementação relativamente simples, e que pode levar a resultados muito bons em determinados problemas de classificação. Este consiste em aplicar o teorema de Bayes, com a prerrogativa de independencia condicional entre cada par de atributos dado o valor da classe.\n",
        "\n",
        "Clique nos links para mais informações sobre o algoritmo [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes) e sobre [classificação de textos](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html). com scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMvflrmwLnJl"
      },
      "source": [
        "# Exemplo: classificação de texto\n",
        "\n",
        "Neste exemplo, utilizaremos um dataset chamado 20newsgroups. \n",
        "\n",
        "O conjunto de dados é uma coleção de aproximadamente 20.000 documentos de grupos de notícias, particionados (quase) uniformemente em 20 grupos/categorias de notícias diferentes. Mais informações sobre esta base podem ser obtidas no repositório [UCI](http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups)\n",
        "\n",
        "As categorias existentes são:\n",
        "\n",
        "* 'alt.atheism',\n",
        "* 'comp.graphics',\n",
        "* 'comp.os.ms-windows.misc',\n",
        "* 'comp.sys.ibm.pc.hardware',\n",
        "* 'comp.sys.mac.hardware',\n",
        "* 'comp.windows.x',\n",
        "* 'misc.forsale',\n",
        "* 'rec.autos',\n",
        "* 'rec.motorcycles',\n",
        "* 'rec.sport.baseball',\n",
        "* 'rec.sport.hockey',\n",
        "* 'sci.crypt',\n",
        "* 'sci.electronics',\n",
        "* 'sci.med',\n",
        "* 'sci.space',\n",
        "* 'soc.religion.christian',\n",
        "* 'talk.politics.guns',\n",
        "* 'talk.politics.mideast',\n",
        "* 'talk.politics.misc',\n",
        "* 'talk.religion.misc'\n",
        "\n",
        "Neste exemplo, vamos considerar apenas duas categorias: '**alt.med**' e '**comp.graphics**'. O scitkit contém uma função que auxilia o download desta base:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeOVilr9NIUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec92530e-0fea-4f33-8df2-32f92315913c"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Categorias selecionadas\n",
        "categories = [\n",
        "    'comp.graphics',\n",
        "    'sci.med',\n",
        "]\n",
        "\n",
        "print(\"Carregando 20 newsgroups dataset para as categorias:\")\n",
        "print(categories)\n",
        "\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test  = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)\n",
        "print(\"%d documents\" % len(twenty_train.filenames))\n",
        "print(\"%d categories\" % len(twenty_train.target_names))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Carregando 20 newsgroups dataset para as categorias:\n",
            "['comp.graphics', 'sci.med']\n",
            "1178 documents\n",
            "2 categories\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X4g3K1-UlcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ce0b9ff-beaf-4686-a316-d01272232419"
      },
      "source": [
        "# Visualizar os dados coletados\n",
        "print(twenty_train.keys())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC-a8vReVU70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6570905-cb22-4c1b-aa96-6dd9c8e8931c"
      },
      "source": [
        "# Visualizando um dos documentos\n",
        "print(twenty_train['data'][0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From: zyeh@caspian.usc.edu (zhenghao yeh)\n",
            "Subject: Re: Need polygon splitting algo...\n",
            "Organization: University of Southern California, Los Angeles, CA\n",
            "Lines: 25\n",
            "Distribution: world\n",
            "NNTP-Posting-Host: caspian.usc.edu\n",
            "Keywords: polygons, splitting, clipping\n",
            "\n",
            "\n",
            "In article <1qvq4b$r4t@wampyr.cc.uow.edu.au>, g9134255@wampyr.cc.uow.edu.au (Coronado Emmanuel Abad) writes:\n",
            "|> \n",
            "|> The idea is to clip one polygon using another polygon (not\n",
            "|> necessarily rectangular) as a window.  My problem then is in\n",
            "|> finding out all the new vertices of the resulting \"subpolygons\"\n",
            "|> from the first one.  Is this simply a matter of extending the\n",
            "|> usual algorithm whereby each of the edges of one polygon is checked\n",
            "|> against another polygon???  Is there a simpler way??\n",
            "|> \n",
            "|> Comments welcome.\n",
            "|> \n",
            "|> Noel.\n",
            "\n",
            "\tIt depends on what kind of the polygons. \n",
            "\tConvex - simple, concave - trouble, concave with loop(s)\n",
            "\tinside - big trouble.\n",
            "\n",
            "\tOf cause, you can use the box test to avoid checking\n",
            "\teach edges. According to my experience, there is not\n",
            "\ta simple way to go. The headache stuff is to deal with\n",
            "\tthe special cases, for example, the overlapped lines.\n",
            "\n",
            "\tYeh\n",
            "\tUSC\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKqq08LkczeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e379a8d8-c5f0-4ef5-dfa6-74676004aca8"
      },
      "source": [
        "# tamanho (sem preprocessamento)\n",
        "len(twenty_train['data'][0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1150"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1UM10zQPV_k"
      },
      "source": [
        "## Preprocessamento\n",
        "\n",
        "Como etapa de préprocessamento, iremos apenas nos limitar a remover palavras muito comuns, as chamadas *stopwords*. Uma lista de possíveis stopwords para inglês encontra-se abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOtv0Xd7Pj8m"
      },
      "source": [
        "stopwords = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n",
        "             'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by',  'can', \"can't\", 'cannot', 'could', \"couldn't\", \n",
        "             'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during','each', 'few', 'for', 'from', 'further', \n",
        "             'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\",\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\",'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", \n",
        "             'it', \"it's\", 'its', 'itself','1st', '2nd', '3rd','4th', '5th', '6th', '7th', '8th', '9th', '10th'\n",
        "             \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself','no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n",
        "             'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', \n",
        "             'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \n",
        "             \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', \n",
        "             'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
        "             \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\", \n",
        "             'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', \n",
        "             'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1ghOFTgKtzh"
      },
      "source": [
        "O código abaixo utiliza a biblioteca NLTK para fazer o pré-processamento dos textos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRtr11vNRMof",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69868ad3-7d10-40d0-8a76-0ef9390b6b71"
      },
      "source": [
        "import string\n",
        "from nltk.tokenize.regexp import RegexpTokenizer\n",
        "\n",
        "def preprocess(text):\n",
        "  \n",
        "  # remover pontuações\n",
        "  text   = text.translate(string.punctuation)\n",
        "  \n",
        "  # converter para lowercase\n",
        "  text = text.lower()\n",
        "  \n",
        "  # tokenizar o texto em palavras\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize(text.lower())\n",
        "\n",
        "  # filtrar palavras\n",
        "  tokens = [word for word in tokens\n",
        "            if word not in stopwords       # descartar stopwords\n",
        "                and len(word) > 3          # descartar palavras com menos de 3 caracteres\n",
        "                and not word[0].isdigit()] # descartar tokens contendo apenas numeros\n",
        "\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "X_train = []\n",
        "for doc in twenty_train['data']:\n",
        "  X_train.append(preprocess(doc))\n",
        "  \n",
        "X_test = []\n",
        "for doc in twenty_test['data']:\n",
        "  X_test.append(preprocess(doc))\n",
        "  \n",
        "print(X_train[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "zyeh caspian zhenghao subject need polygon splitting algo organization university southern california angeles lines distribution world nntp posting host caspian keywords polygons splitting clipping article wampyr g9134255 wampyr coronado emmanuel abad writes idea clip polygon using another polygon necessarily rectangular window problem finding vertices resulting subpolygons first simply matter extending usual algorithm whereby edges polygon checked another polygon simpler comments welcome noel depends kind polygons convex simple concave trouble concave loop inside trouble cause test avoid checking edges according experience simple headache stuff deal special cases example overlapped lines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxcZX12qc5Sr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fa6cdc-8d73-45a4-c5e3-3d25c30509c5"
      },
      "source": [
        "# tamanho (com preprocessamento)\n",
        "len(X_train[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "697"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aH2nkY3Sv1N"
      },
      "source": [
        "### Exercício\n",
        "\n",
        "Converter o a função de preprocessamento acima para utilizar o SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h02NOJ7US8Dc"
      },
      "source": [
        "# sua resposta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwUHXRvfMOc-"
      },
      "source": [
        "## Representação vetorial do texto\n",
        "\n",
        "Neste exemplo, utilizaremos o algoritmo Naive Bayes para classificar documentos de texto em categorias. Para isso, precisamos antes converter o texto para uma representação vetorial, ou seja, cada documento/exemplo precisa ser representado por um vetor de dimensões pré-definidas. Utilizaremos três técnicas básicas: BOW (*Bag of Words*), TF (*Term Frequency*) e TF-IDF (*Term Frequency - Inverse Document Frequency*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6o5yjjaPJCG"
      },
      "source": [
        "### BOW\n",
        "\n",
        "Consiste basicamente em contar quantas vezes cada palavra aparece no documento. Ou seja, sua aplicação a um conjunto de *n* documentos, produz uma matriz *n x d*, onde *d* corresponde ao tamanho do vocabulário considerado. No Scikit, esta representação é implementada pelo [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMmOZfv2SsZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f25b3b46-0ee4-477b-eaba-b7130e80a407"
      },
      "source": [
        "# Exemplo de uso do BOW no scikit\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtimrxTfTs8T"
      },
      "source": [
        "Aplicando ao nosso dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkcoN3_XTu-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ada6839-cb6b-4125-e8d2-e92911c50bbe"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "bow_model  = vectorizer.fit(X_train)\n",
        "\n",
        "X_bow_train = bow_model.transform(X_train)\n",
        "X_bow_test  = bow_model.transform(X_test)\n",
        "\n",
        "print(X_bow_train.shape,X_bow_test.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1178, 19924) (785, 19924)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB-lp5d-Wej9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcdab584-9ac1-420b-84a9-21975424a31a"
      },
      "source": [
        "# matriz está armazenada em formato sparse\n",
        "print(X_bow_train[0,:])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 171)\t1\n",
            "  (0, 273)\t1\n",
            "  (0, 678)\t1\n",
            "  (0, 681)\t1\n",
            "  (0, 906)\t1\n",
            "  (0, 959)\t2\n",
            "  (0, 1204)\t1\n",
            "  (0, 1448)\t1\n",
            "  (0, 2555)\t1\n",
            "  (0, 2725)\t1\n",
            "  (0, 2728)\t2\n",
            "  (0, 2754)\t1\n",
            "  (0, 2956)\t1\n",
            "  (0, 2958)\t1\n",
            "  (0, 3251)\t1\n",
            "  (0, 3256)\t1\n",
            "  (0, 3463)\t1\n",
            "  (0, 3643)\t2\n",
            "  (0, 3919)\t1\n",
            "  (0, 3998)\t1\n",
            "  (0, 4513)\t1\n",
            "  (0, 4731)\t1\n",
            "  (0, 5176)\t1\n",
            "  (0, 5584)\t2\n",
            "  (0, 5762)\t1\n",
            "  :\t:\n",
            "  (0, 14577)\t1\n",
            "  (0, 15012)\t1\n",
            "  (0, 16149)\t2\n",
            "  (0, 16150)\t1\n",
            "  (0, 16157)\t1\n",
            "  (0, 16487)\t1\n",
            "  (0, 16532)\t1\n",
            "  (0, 16631)\t2\n",
            "  (0, 17018)\t1\n",
            "  (0, 17043)\t1\n",
            "  (0, 17059)\t1\n",
            "  (0, 17716)\t1\n",
            "  (0, 18251)\t2\n",
            "  (0, 18599)\t1\n",
            "  (0, 18748)\t1\n",
            "  (0, 18752)\t1\n",
            "  (0, 18968)\t1\n",
            "  (0, 19232)\t2\n",
            "  (0, 19367)\t1\n",
            "  (0, 19418)\t1\n",
            "  (0, 19499)\t1\n",
            "  (0, 19623)\t1\n",
            "  (0, 19669)\t1\n",
            "  (0, 19890)\t1\n",
            "  (0, 19921)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6COLPTEVpJm"
      },
      "source": [
        "### *Term Frequency* (TF)\n",
        "A contagem de ocorrências (i.e., BOW) é um bom começo, mas há um problema: documentos mais longos terão valores de contagem média mais altos do que documentos mais curtos, embora possam falar sobre os mesmos tópicos.\n",
        "\n",
        "Para evitar essas possíveis discrepâncias, basta dividir o número de ocorrências de cada palavra em um documento pelo número total de palavras no documento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbJtPIvSVtke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d111cfa5-168c-4d7b-dd1b-b55ad96713ca"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=False)\n",
        "tf_model = vectorizer.fit(X_train)\n",
        "\n",
        "X_tf_train = tf_model.transform(X_train)\n",
        "X_tf_test  = tf_model.transform(X_test)\n",
        "\n",
        "print(X_tf_train[0,:])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 171)\t0.08804509063256238\n",
            "  (0, 273)\t0.08804509063256238\n",
            "  (0, 678)\t0.08804509063256238\n",
            "  (0, 681)\t0.08804509063256238\n",
            "  (0, 906)\t0.08804509063256238\n",
            "  (0, 959)\t0.17609018126512477\n",
            "  (0, 1204)\t0.08804509063256238\n",
            "  (0, 1448)\t0.08804509063256238\n",
            "  (0, 2555)\t0.08804509063256238\n",
            "  (0, 2725)\t0.08804509063256238\n",
            "  (0, 2728)\t0.17609018126512477\n",
            "  (0, 2754)\t0.08804509063256238\n",
            "  (0, 2956)\t0.08804509063256238\n",
            "  (0, 2958)\t0.08804509063256238\n",
            "  (0, 3251)\t0.08804509063256238\n",
            "  (0, 3256)\t0.08804509063256238\n",
            "  (0, 3463)\t0.08804509063256238\n",
            "  (0, 3643)\t0.17609018126512477\n",
            "  (0, 3919)\t0.08804509063256238\n",
            "  (0, 3998)\t0.08804509063256238\n",
            "  (0, 4513)\t0.08804509063256238\n",
            "  (0, 4731)\t0.08804509063256238\n",
            "  (0, 5176)\t0.08804509063256238\n",
            "  (0, 5584)\t0.17609018126512477\n",
            "  (0, 5762)\t0.08804509063256238\n",
            "  :\t:\n",
            "  (0, 14577)\t0.08804509063256238\n",
            "  (0, 15012)\t0.08804509063256238\n",
            "  (0, 16149)\t0.17609018126512477\n",
            "  (0, 16150)\t0.08804509063256238\n",
            "  (0, 16157)\t0.08804509063256238\n",
            "  (0, 16487)\t0.08804509063256238\n",
            "  (0, 16532)\t0.08804509063256238\n",
            "  (0, 16631)\t0.17609018126512477\n",
            "  (0, 17018)\t0.08804509063256238\n",
            "  (0, 17043)\t0.08804509063256238\n",
            "  (0, 17059)\t0.08804509063256238\n",
            "  (0, 17716)\t0.08804509063256238\n",
            "  (0, 18251)\t0.17609018126512477\n",
            "  (0, 18599)\t0.08804509063256238\n",
            "  (0, 18748)\t0.08804509063256238\n",
            "  (0, 18752)\t0.08804509063256238\n",
            "  (0, 18968)\t0.08804509063256238\n",
            "  (0, 19232)\t0.17609018126512477\n",
            "  (0, 19367)\t0.08804509063256238\n",
            "  (0, 19418)\t0.08804509063256238\n",
            "  (0, 19499)\t0.08804509063256238\n",
            "  (0, 19623)\t0.08804509063256238\n",
            "  (0, 19669)\t0.08804509063256238\n",
            "  (0, 19890)\t0.08804509063256238\n",
            "  (0, 19921)\t0.08804509063256238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjQ0ITVjX8fV"
      },
      "source": [
        "### TF-IDF\n",
        "Palavras que aparecem em muitos documentos podem não ser interessantes para diferencia a classe desses documentos. Portanto, podemos dar um peso maior as palavras que aparecem em poucos documentos, pois essas palavras funcionam melhor para diferenciar as classes entre os documentos. TF-IDF é uma indicação do poder de discriminação do termo. O Log é usado para diminuir o efeito em relação a tf.\n",
        "\n",
        "Usando o scikit, basta ativar o flag `use_idf=True`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWUoJsm_YFG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e6c9be7-3119-42cb-d163-b1fa0a42734f"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=True)\n",
        "tfidf_model = vectorizer.fit(X_train)\n",
        "\n",
        "X_tfidf_train = tfidf_model.transform(X_train)\n",
        "X_tfidf_test  = tfidf_model.transform(X_test)\n",
        "\n",
        "print(X_tfidf_train[0,:])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 19921)\t0.10383939382532555\n",
            "  (0, 19890)\t0.10383939382532555\n",
            "  (0, 19669)\t0.032622129876398155\n",
            "  (0, 19623)\t0.049784186588151064\n",
            "  (0, 19499)\t0.08318936336659238\n",
            "  (0, 19418)\t0.11493120439343397\n",
            "  (0, 19367)\t0.09587341399898087\n",
            "  (0, 19232)\t0.22986240878686795\n",
            "  (0, 18968)\t0.10224715376104548\n",
            "  (0, 18752)\t0.08674226639616602\n",
            "  (0, 18748)\t0.057554115504060505\n",
            "  (0, 18599)\t0.03552235596093959\n",
            "  (0, 18251)\t0.16637872673318477\n",
            "  (0, 17716)\t0.07373432736258789\n",
            "  (0, 17059)\t0.12761525502582247\n",
            "  (0, 17043)\t0.018299216945731396\n",
            "  (0, 17018)\t0.07219567215927884\n",
            "  (0, 16631)\t0.2365350922301801\n",
            "  (0, 16532)\t0.07847129256054856\n",
            "  (0, 16487)\t0.0994263170285545\n",
            "  (0, 16157)\t0.07889198274202017\n",
            "  (0, 16150)\t0.10751151041623824\n",
            "  (0, 16149)\t0.1438057568835491\n",
            "  (0, 15012)\t0.09034191074603323\n",
            "  (0, 14577)\t0.10383939382532555\n",
            "  :\t:\n",
            "  (0, 5762)\t0.11493120439343397\n",
            "  (0, 5584)\t0.18767614394434426\n",
            "  (0, 5176)\t0.04812701724127763\n",
            "  (0, 4731)\t0.09482745978384977\n",
            "  (0, 4513)\t0.08115402133978364\n",
            "  (0, 3998)\t0.11493120439343397\n",
            "  (0, 3919)\t0.10078243488840369\n",
            "  (0, 3643)\t0.22986240878686795\n",
            "  (0, 3463)\t0.0848797227916673\n",
            "  (0, 3256)\t0.11493120439343397\n",
            "  (0, 3251)\t0.10966684773824122\n",
            "  (0, 2958)\t0.11826754611509005\n",
            "  (0, 2956)\t0.09289944485031307\n",
            "  (0, 2754)\t0.06438661845185546\n",
            "  (0, 2728)\t0.2076787876506511\n",
            "  (0, 2725)\t0.07889198274202017\n",
            "  (0, 2555)\t0.07472371517426553\n",
            "  (0, 1448)\t0.08115402133978364\n",
            "  (0, 1204)\t0.03334934677185028\n",
            "  (0, 959)\t0.1234077793452408\n",
            "  (0, 906)\t0.09289944485031307\n",
            "  (0, 681)\t0.07576966938939664\n",
            "  (0, 678)\t0.11826754611509005\n",
            "  (0, 273)\t0.09200662305135877\n",
            "  (0, 171)\t0.11493120439343397\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTJq9ya-Yw0z"
      },
      "source": [
        "# Treinando modelo NaiveBayes (NB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYV00SwlbDhF"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf = MultinomialNB()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW8Q3aN-axP_"
      },
      "source": [
        "## BOW + NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDOh6kBpbAN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67199564-ae50-495b-c954-8a1d7f249267"
      },
      "source": [
        "clf.fit(X_bow_train, twenty_train.target)\n",
        "\n",
        "acc = clf.score(X_bow_test , twenty_test.target)\n",
        "print('Acurácia: ', acc)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia:  0.9694267515923567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PFI2MQ-a0_4"
      },
      "source": [
        "## TF + NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09QMVfd0buzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71506272-b1b9-42a9-dad3-190ec0b2ad71"
      },
      "source": [
        "clf.fit(X_tf_train, twenty_train.target)\n",
        "\n",
        "acc = clf.score(X_tf_test , twenty_test.target)\n",
        "print('Acurácia: ', acc)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia:  0.9477707006369427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgfiTAx8a2zk"
      },
      "source": [
        "## TF-IDF + NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y24I7yk0Y2Wh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45cc14f-97cd-4e92-d09d-3fe178c06395"
      },
      "source": [
        "clf.fit(X_tfidf_train, twenty_train.target)\n",
        "\n",
        "acc = clf.score(X_tfidf_test , twenty_test.target)\n",
        "print('Acurácia: ', acc)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia:  0.9579617834394905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzxDimJCaN-x"
      },
      "source": [
        "# Testando iterativamente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BictP9Z9Z35L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "babda27d-5d56-4748-e771-b1ad8d952c57"
      },
      "source": [
        "docs = ['The patient is getting well', 'OpenGL on the GPU is fast']\n",
        "preprocessed_docs = [preprocess(doc) for doc in docs]\n",
        "\n",
        "print('\\nafter preprocessing:')\n",
        "print(preprocessed_docs)\n",
        "\n",
        "docs_preds = clf.predict(tfidf_model.transform(preprocessed_docs))\n",
        "\n",
        "print('\\npredictions:')\n",
        "for i,doc in enumerate(docs):\n",
        "    print('{} -> {}'.format(doc, twenty_train.target_names[docs_preds[i]]))\n",
        "    "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "after preprocessing:\n",
            "['patient getting well', 'opengl fast']\n",
            "\n",
            "predictions:\n",
            "The patient is getting well -> sci.med\n",
            "OpenGL on the GPU is fast -> comp.graphics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHy4D2kMaULb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}